---
title: "Homework 6"
author: "Li Zhang"
date: "07/07/2021"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r,echo=FALSE}
options(digits=8)
```


# Instructions

There are only 3 exercises for this homework; these will be challenging enough that you don't need four. You will get 10 point bonus for completing the exercises.

*Warning* I will continue restricting the use of external libraries in R, particularly `tidyverse` libraries. You may choose to use `ggplot2`, but take care that the plots you produce are at least as readable as the equivalent plots in base R. You will be allowed to use whatever libraries tickle your fancy in the final project.


## Reuse

For many of these exercises, you may be able to reuse functions written in prior homework. Define those functions here. I'm also including data vectors that can be used in some exercises.

```{r}
CaloriesPerServingMean <- c(268.1, 271.1, 280.9, 294.7, 285.6, 288.6, 384.4)
CaloriesPerServingSD <- c(124.8, 124.2, 116.2, 117.7, 118.3, 122.0, 168.3)
Year <- c(1936, 1946, 1951, 1963, 1975, 1997, 2006)

#Function norm.pdf
norm.pdf<-function(x, mu=0, sigma=1){
m<- exp(-((x-mu)^2)/(2*(sigma)^2))
n<- 1/(sigma*(sqrt(2*pi)))
return(m*n)
}

```

# Exercise 1

## Part a.

Write a function or macro to compute mean, standard deviation, skewness and kurtosis from a single vector of numeric values. You can use the built-in mean function, but must use one (and only one) for loop to compute the rest. Be sure to include a check for missing values. Note that computationally efficient implementations of moments take advantage of $(Y_i-\bar{Y})^4 = (Y_i-\bar{Y}) \times (Y_i-\bar{Y})^3$, etc.


See https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm for formula for skewness and kurtosis. This reference gives several definitions for both skewness and kurtosis, you only need to implement one formula for each. Note that for computing skewness and kurtosis, standard deviation is computed using $N$ as a divisor, not $N-1$.


Your function should return a list with `Mean`, `SD`, `Skewness` and `Kurtosis`. If you use IML, you will need to implement this as a subroutine and use call by reference; include these variables in parameter list.

```{r}

compute.result<-function(data){

 sum.data<-0
 n<-0
 for(i in 1: length(data)){
   if(!is.na(data[i])){
     sum.data<-sum.data+data[i]
     n<-n+1
   }
 }
mean.data<-sum.data/n

SS <- 0
n<-0
SS_skewness<-0
SS_kurtosis<-0
for(i in 1:length(data)) {
  if(!is.na(data[i])) {
    SS <- SS+(data[i]-mean.data)^2
    SS_skewness <- SS_skewness+(data[i]-mean.data)^3
    SS_kurtosis <- SS_kurtosis+(data[i]-mean.data)^4
    n <- n+1
  }
}

sd<-sqrt(SS/(n-1))
sd1<-sqrt(SS/n)
g1<-(SS_skewness/n)/(sd1^3)
kurtosis<-(SS_kurtosis/n)/(sd1^4)
list(Mean=mean.data, SD=sd, Skewness=g1, Kurtosis=kurtosis )
}


```

## Part b.

Test your function by computing moments for `Mean55` from `Khan.csv`, for `ELO` from `elo.csv` or the combine observations from `SiRstvt`.

```{r}
table1=read.csv("Khan.csv", header=TRUE)
compute.result(table1$Mean55)
table2=read.csv("elo.csv", header=TRUE)
compute.result(table2$ELO)

```

If you wish, compare your function results with the `skewness` and `kurtosis` in the `moments` package.

```{r,eval=TRUE}
library(moments)
test.data1<-function(data_table){
  m<-mean(data_table)
  s<-sd(data_table)
  sk<-skewness(data_table)
  k<-kurtosis(data_table)
  list(Mean=m, SD=s, Skewness=sk, Kurtosis=k )
}
test.data1(table1$Mean55)
test.data1(table2$ELO)


```

# Exercise 2

Consider Newton's method to find a minimum or maximum value attained by a function over an interval. Given a function $f$, we wish to find

$$
\max_{x \in [a,b]} f(x)
$$

Start with an initial guess, $x_0$, then generate a sequence of guesses using the formula

$$
x_{k+1} = x_{k} - \frac{f'(x_{k})}{f''(x_{k})}
$$

where $f'$ and $f''$ are first and second derivatives. We won't be finding derivatives analytically, instead, we will be using numerical approximations (*central finite differences*), given by

$$
\begin{aligned}
f' & \approx \frac{f(x+\frac{h}{2}) - f(x-\frac{h}{2})}{h} \\
f'' & \approx \frac{f(x+h) - 2f(x)+f(x-h)}{h^2}
\end{aligned}
$$
where $h$ is some arbitrary small value.

We will work with the normal pdf, $f (x ; \mu, \sigma^2) = \frac{1}{\sigma \sqrt{2 \pi}^{}} e^{- \frac{(x - \mu)^2}{2 \sigma^2}}$. Let $\mu = m_{1936}$ be the mean Calories per Serving from 1936, and let $\sigma = s_{1936}$ be the corresponding standard deviation. We will wish to find the $x_*$ that maximizes

$$
\max pdf (x ; m_{1936}, s_{1936}^2)
$$

Let the initial guess be $x_0 = 180$ and let $h = 0.1$. Calculate 10 successive $x_k$, saving each value in a vector. Print the final $x_k$. Why does this value maximize the likelihood function?

```{r}

mean_1936=268.1
sigma_1936=124.8
guess.x<-rep(0,10)
guess.x[1]=180
h=0.1
iteration_number=10

for (k in 1:(iteration_number-1)){
fprime<-((norm.pdf((guess.x[k]+h/2),mean_1936,sigma_1936))
         -norm.pdf((guess.x[k]-h/2),mean_1936,sigma_1936))/h

f2<-(norm.pdf((guess.x[k]+h),mean_1936,sigma_1936)
     -2*norm.pdf(guess.x[k],mean_1936,sigma_1936)
     +norm.pdf((guess.x[k]-h),mean_1936,sigma_1936))/(h^2)

guess.x[k+1]<-(guess.x[k]- fprime/f2)

}  


# The reason why the final guess xk maximize the likelihood function.
if(abs(guess.x[iteration_number]-guess.x[iteration_number-1])<10^-6){
  print("the difference between final two guesses is less than 10^-6")
  
}else{
  print("the difference between final two guesses is greater than 10^-6")
}

# First, this is because the difference between final two guesses 
# is less than 10^-6.
# Second, as we can see the sequence of the guesses, 
# when the guess becomes 268.1, 
# it never changed.

```

### Part b.

Plot the sequence of $x$ versus iteration number ($k$) as the independent variable. Add a horizontal line corresponding to $m_{1936}$. How many iterations are required until $|x_{k+1} - x_{k}| < 10^{-6}$?

```{r}

k=seq(from= 1 ,to= 10, by=1)
plot(k, guess.x[k], xlab="k", ylab="x/(m1936)",main="Newton's method", 
     type="l", col='blue')

abline(h=mean_1936, col="red")
legend(x="topright", legend=c("x","m1936"), col=c("blue","red"), 
       lwd=1, cex=1, horiz="FALSE", text.font=2)

count=0
for(k in 1:9){
    if(abs(guess.x[k+1]-guess.x[k])>=10^(-6)){
          count<-count+1
    }
   else{
     break
   }
}
count




```


# Exercise 3

Consider the Trapezoidal Rule for integration. From "Analysis by Its History" (https://books.google.com/books/about/Analysis_by_Its_History.html?id=E2IhMXPZMNIC)

> On the interval $\left[ x_i, x_{i+1}\right]$ the function $f(x)$ is replaced by a straight line passing through $\left(x_i,f(x_i)\right)$ and $\left(x_{i+1},f(x_{i+1})\right)$. The integral between $x_i$ and $x_{i+1}$ is then approximated by the trapezoidal area $h \cdot \left(f(x_i)+f(x_{i+1})\right)/2$ and we obtain
 
$$
\int _{a} ^{b} f(x) dx = F(x) \approx \sum _{i=1} ^{N-1} \frac{h}{2} \left(f(x_i)+f(x_{i+1})\right)
$$

We will calculate the integral for the normal pdf 

$$
\int _{-3} ^{3}  L (x ; \mu, \sigma^2) dx = \int _{-3} ^{3} \frac{1}{\sigma \sqrt{2 \pi}^{}} e^{- \frac{(x - \mu)^2}{2 \sigma^2}} dx
$$

with $\mu=0$ and $sigma=1$, using your `norm.pdf` function. We will do this by creating a sequence of approximations, each more precise than the preceding approximation.

### Part a.

Calculate a first approximation of step size $h_0=1$, using the sequence of $x_i = \left\{-2.0,-1.0,0.0,1.0,2.0\right\}$. Let this approximation be $F_0$. Print the first approximation.

```{r}

h0=1
x_i=c(-2.0, -1.0, 0.0,1.0, 2.0)
N=length(x_i)
F_0=0
  for (k in 1:(N-1)){
    F_0<-F_0 + ((h0/2)*(norm.pdf(x_i[k], 0,1)+norm.pdf(x_i[k+1],0,1)))
  }
 F_0
 
 

```


### Part b.

Continue to calculate a series of approximations $F_0, F_1, F_2, \dots$ such that $F_{k+1}$ improves on $F_k$ by increasing $N$. Do this by decreasing the step size by 2, $h_{k+1} = h_{k}/2$. Thus, the sequence used to calculate $F_1$ will be of the form $x_i = \left\{-2.0, -1.5, -1.0, \dots, 1.5, 2.0 \right\}$ 

Calculate the first 10 approximations in the series and print the final approximation. 

```{r}
h0=1
x_i=c(-2.0, -1.0, 0.0,1.0, 2.0)
N=length(x_i)
my_result<-vector(mode="numeric")
for(K in 0: 9){
  F_K=0
  for(k in 1:(N-1)){
    
      F_K<-F_K + ((h0/2)*(norm.pdf(x_i[k], 0,1)+norm.pdf(x_i[k+1],0,1)))
  }

  my_result<-append(my_result, F_K)
  F_final=F_K
  h0<-h0/2
  x_i<-seq(-2.0,2.0, by=h0)
  N=length(x_i)
 
}
# first 10 approximations
print(my_result)
#final approximation
print(F_final)
```

### Part c.

Plot the successive approximations $F_i$ against iteration number (you will need to define an array to store each approximation). Add a horizontal line for the expected value (`pnorm(2, lower.tail = TRUE)-pnorm(-2, lower.tail = TRUE)`). Set y-axis limits for this plot to be $[0.92,0.96]$ to best view the progression of approximations.

It is common practice to terminate a sequence of approximations when the difference between successive approximations is less than some small value. What is the difference between your final two approximations (It should be less than $10^{-6}$)?

```{r}
#define an Array to store each approximation
F_result<-array(my_result,dim=c(1,10,1))
F_result

print(pnorm(2, lower.tail = TRUE)-pnorm(-2, lower.tail = TRUE))

k=seq(from= 1 ,to= 10, by=1)

plot(k, F_result, xlab="k", ylab="F_k(approxiations)/ pnorm",
     main="Numerical Integration", type="l", col='blue',
     ylim=c(0.92, 0.96))
abline(h=(pnorm(2, lower.tail = TRUE)-pnorm(-2, lower.tail = TRUE)),
       col="red")
legend(x="bottomright", legend=c("F_k(approxiations)","pnorm"),   
       col=c("blue","red"), lwd=1, cex=1, 
       horiz="FALSE", text.font=2)

difference_final2<-(F_result[1,(length(F_result)),1]-F_result[1,(length(F_result)-1),1])
difference_final2

if(difference_final2<10^(-6)){
 print("The difference between final appoximation is less than 10^-6") 
}else{ 
      print("the difference between final appoximation is grater than or equal to 10^-6") }

#Yes, the difference between the final two approximations is less than 10^-6.

# ---***Worth to mention***---
# When I calculate the results in R studio
# They are all automatically rounded to 7 decimal points.
# As a result, in R studio, 
# final aproximation F_9 is equal to 
# pnorm(2, lower.tail = TRUE)-pnorm(-2, lower.tail = TRUE)
# which is 0.9544997
# however, when I knit to PDF, thing becomes weird. 
# *The output in R studio is different from the one in PDF.*
# this makes me so confused for a while, then I notice that 
# since we set: options(digits=8) at the beginning
# when I knit to PDF, F_9 becomes 0.95449967 
# pnorm(2, lower.tail = TRUE)-pnorm(-2, lower.tail = TRUE)
# becomes 0.95449974.
# If I choose to set : options(digits=7)
# The values in PDF would be exactly the same as the values in R studio.
# In this exercise, I left options(digits=8) as it is.
```




